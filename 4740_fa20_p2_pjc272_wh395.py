# -*- coding: utf-8 -*-
"""4740_FA20_p2_pjc272_wh395.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rDAx_4IZglhFYNiFofnisuOAys-ZS-GR

# Project 2: Span Identification with Sequence Labeling Models
## CS4740/5740 Fall 2020

### Project Submission Due: Oct 23rd
Please submit **pdf file** of this notebook on **Gradescope**, and **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 instructions.

Names: Wesley Ho, Phil Cipollina

Netids: wh395, pjc272

Don't forget to share your newly copied notebook with your partner!


**Reminder: both of you can't work in this notebook at the same time from different computers/browser windows because of sync issues. We even suggest to close the tab with this notebook when you are not working on it so your partner doesn't get sync issues.**

### Q0: Individual Member Contribution

Briefly explain the contribution of individual group members here. Report if working loads are unfairly distributed.

Work was done together over discord as pair programming

# Overview

---

In this project, you will implement a model that identifies relevant information in a text and tags it with the appropriate label. Particularly, the task of this project is **Propaganda Identification**. The given dataset contains (manual) annotations indicating fragments of text that exhibit one of a set of well-known [propaganda techniques](https://propaganda.qcri.org/annotations/definitions.html). Your task is to develop NLP models to identify these propagandistic spans of text automatically. We will treat this as a **sequence-tagging task**: for each token in the input text, assign a label $y\in\{0,1\}$, such that *1 represents propagandistic text* and *0 represents non-propaganda*.   (A description of the original task formulation is [here](https://propaganda.qcri.org/ptc/).  We are working on a modified version of their "span identification" task.)

For this project, you will implement two sequence labeling approaches:

- Model 1 : a Hidden Markov Model (HMM)
- Model 2 : a Maximum Entropy Markov Model (MEMM), which is an adaptation of an HMM in which a Logistic Regression classifier (also known as a MaxEnt classifier) is used to obtain the lexical generation probabilities (i.e., the observation/emission probability matrix, so "observations" == "emissions" == "lexical generations").  Feature engineering is strongly suggested. (Papers from the [Workshops on Figurative Language Processing](https://sites.google.com/view/figlang2020/) can provide good insights on feature selection for this task.) You can also refer to the J&M book. 

Implementation of the Viterbi algorithm (for finding the most likely tag sequence to assign to an input text) is required for both parts, so make sure that you understand it ASAP.

You will implement and train two sequence tagging models, generate your predictions for the provided test set, and submit them to **Kaggle**. Please enter all code and answer the questions of this colab notebook.

**Jurafsky & Martin reading on HMMs and MEMMs can be found in Ch. 8.3–8.5.** The code you write can be added anywhere in the document, but we implore you to keep it readable. You will be asked to describe and motivate your models in parts of the document. You will be graded on both the code and text you write; see grading details at the end of the document.

# Notes

---

1. Please read through the entire notebook before you start coding. That might inform your code structure.
2. Grading breakdown is found at the end; please consult it.
3. Google colab does **not** provide good synchronization; we do not recommend multiple people to work on the same notebook at the same time.
4. The project is somewhat open ended. ("But that's a good thing.  Really. It's more fun that way", says Claire and Esin.) We will ask you to implement some model, but precise data structures and so on can be chosen by you. However, to integrate with Kaggle, you will need to submit Kaggle predictions using our tokenization code.  As a result, **it is probably easiest if you use our tokenization code for the entire project**.
5. You will be asked to fill in your code at various points of the document. You will also be asked to answer questions that analyze your results and motivate your implementation. These questions are named Q1-Q8. You may create additional cells to insert code, text and images as necessary.
6. Kaggle is not able to calculate *span-level* P/R/F1 measures, which is the standard way to evaluate this type of sequence-tagging task. And we don't actually care so much about the token-level tagging accuracy, which Kaggle **can** calculate.  In particular, there are many fewer propaganda tokens than non-propaganda ones, so always guessing "non-propaganda" would produce a very high accuracy.  So we are compromising by using token-level **weighted accuracy**.  Here is how it works:

A **weighted accuracy** metric that favors finding propagandistic tokens over non-propagandistic ones. The weights for both classes are the inverse of their frequencies.  

``` 
frac_propaganda = num_propaganda/num_labels   [in the answer key]
weight_propaganda = 1/frac_propaganda  
weight_non_propaganda = 1/(1-frac_propaganda)

weighted_accuracy = 
   ((weight_propaganda * # propaganda correct) 
                      +
   (weight_non_propaganda * # non_propaganda correct)) / 
   
   ((weight_propaganda * num_propaganda) 
                      +
   (weight_non_propaganda * num_non_propaganda))
```  
This is also known as the **macro average**, i.e., the average of the accuracy for each label type.

# Task and dataset

---

1. Obtain the data from Kaggle at https://www.kaggle.com/t/8a8030baefcc4d91b715f114353dba38.
2. Unzip the data. Put it into your google drive, and mount it on colab as per below:
"""

from google.colab import drive
import os
import time
drive.mount('/content/drive', force_remount=True)



train_path = os.path.join(os.getcwd(), "drive", "My Drive/CS 4740/Project 2", "Dataset", "train", "train") # replace based on your Google drive organization
test_path = os.path.join(os.getcwd(), "drive", "My Drive/CS 4740/Project 2", "Dataset", "test", "test") # replace based on your Google drive organization
print(os.listdir(train_path)[:5])
print(os.listdir(test_path)[:5])

"""3. The *train* directory contains *article{XXX}.txt* files which are plaintext documents and also *label* files such as *article{XXX}.task-si.labels*. These label files correspond to the byte-span offests of each segment of propaganda in the associated article. (The tokenizer that we describe just below converts the byte-span representation of propagandistic text spans  into the token-level gold-standard labels that your sequence-tagging models require for training.) The test directory *only* contains articles; you will use your models to detect the propagandistic spans within them.  

4. We provide a tokenizer for these documents. You **must** use this tokenizer as the labels that Kaggle expects are based upon this tokenization. The code below tokenizes each document and generates the appropriate token-level labels consistent with the associated *labels* file. (This is so that you do not need to perform any byte-level text processing.  In particular, the tokenizer  merges nested or overlapping propagandistic text spans from the article into a single segment. You really shouldn't have to look at the *lables* files at all.) The code uses python type annotations; these indicate the type of arguments the functions take.

5. Documents are represented as a list of strings, each being a token. Labels are represented as a list of integers in {0,1}, 1 corresponding to a propagandistic token and 0 to not propaganda.


"""

import os
from typing import List, Tuple

from nltk import word_tokenize, sent_tokenize
import nltk
nltk.download('punkt')

def read_txt(fname):
  with open(fname) as open_article:
    lines = open_article.read()
  return lines

def read_labels(labels : str) -> List[Tuple[int, int]]:
	"processing of labels file"
	labels = labels.split("\n")[:-1]
	labels = [tuple(map(int, l.split("\t")[1:])) for l in labels]
	return labels

def sort_and_merge_labels(labels : List[Tuple[int, int]]) -> List[Tuple[int, int]]:
  "sort labels, necessary for later splitting"
  if len(labels) == 0:
    return labels
  labels = list(sorted(labels, key = lambda t: t[0]))
  # merge
  curr = labels[0]
  merged = []
  for l in labels[1:]:
      # if distinct, add
      if l[0] > curr[1]:
        merged.append(curr)
        curr = l
      # else merge
      else:
        curr = (curr[0], max(curr[1], l[1]))
  merged.append(curr)
  return merged

def split_with_labels(labels : List[Tuple[int, int]], article : str) -> Tuple[List[str], List[int]]:
  "split text into segments based upon labels"
  if len(labels) == 0:
    return [article], [0]
  segments = []
  binary_class = []
  start = 0
  for l_start, l_end in labels:
    std_seg = article[start:l_start]
    prop_seg = article[l_start:l_end]
    segments.append(std_seg)
    binary_class.append(0)
    segments.append(prop_seg)
    binary_class.append(1)
    start = l_end
  last_seg = article[start:]
  segments.append(last_seg)
  binary_class.append(0)
  return segments, binary_class

def remove_newline_fix_punc_seg(segments):
  " preprocessing necessry for tokenization to be consistent"
  segments = [s.replace("\n", " ").replace(".", " .") for s in segments]
  return segments

def remove_newline_fix_punc_art(article):
  " preprocessing necessry for tokenization to be consistent"
  article = article.replace("\n", " ").replace(".", " .")
  return article

def get_toks(input):
  output = []
  for toks in [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(input)]:
    output += toks
  return output

# This is the function you may need to call
def tokenize_article(article_file):
  "calls all functions above and perform sanity checks"
  article = read_txt(article_file)
  article = remove_newline_fix_punc_art(article)
  art_toks = get_toks(article)
  return art_toks

# This is the function you may need to call
def master_tokenizer(article_file, labels_file):
  "calls all functions above and perform sanity checks"
	# read and get labels
  article = read_txt(article_file)
  labels = read_txt(labels_file)
  labels = read_labels(labels)
  labels = sort_and_merge_labels(labels)
  segments, binary_class = split_with_labels(labels, article)
  article = remove_newline_fix_punc_art(article)
  segments = remove_newline_fix_punc_seg(segments)
  # sanity check
  reconstructed = ""
  for seg, lab in zip(segments, binary_class):
    reconstructed += seg
  assert reconstructed == article
	# tokenize
  seg_toks = []
  new_labels = []
  for seg, label in zip(segments, binary_class):
    new_toks = get_toks(seg)
    seg_toks += new_toks
    new_labels += [label for _ in range(len(new_toks))]
	# sanity check
  art_toks = get_toks(article)
  sanity = True
  if len(art_toks) != len(seg_toks):
    sanity = False
  for i, (at, st, lab) in enumerate(zip(art_toks, seg_toks, new_labels)):
    if at != st:
      sanity = False
      break
  return seg_toks, new_labels, sanity

""" 6. Execute the commands below to visualize the tokenization:"""

article_file = "article698018235.txt"
labels_file = "article698018235.task-si.labels"
article_file = os.path.join(train_path, article_file)
labels_file = os.path.join(train_path, labels_file)
tokens, labels, _ = master_tokenizer(article_file, labels_file)

print(len(tokens), len(labels), tokens[:10])
print(labels)

"""7.  Provide some quantitative data exploration. Assess dataset size, documents lengths and class inbalance. Give some examples of sentences containing propaganda techniques."""

count = 0
lst2 = []
while count < len(labels):
  lst = []
  if labels[count] == 1:
    while tokens[count - 1] != "." and count > 0:
      count -= 1
    while tokens[count] != '.':
      lst.append(tokens[count])
      count += 1
    lst2.append(lst)
    count += 1
  else:
    count += 1
for prop in lst2:
  print(prop)
#The output lists are examples of sentences containing propaganda techniques

c = 0
for i in range(len(tokens)):
  c += labels[i]
print(c / len(tokens))
trainp = os.listdir(train_path)
totaltokens = 0
docs = 0
maxds = -2**30
minds = 2**30
d = {}
for i in trainp:
  if i[-4:] == ".txt":
    af = os.path.join(train_path, i)
    ta = tokenize_article(af)
    lta = len(ta)
    totaltokens += lta
    docs += 1
    if maxds < lta:
      maxds = lta
    if minds > lta:
      minds = lta
    for word in ta:
      if word in d:
        d[word] += 1
      else:
        d[word] = 1

print('total words: ', str(totaltokens))
print('average words/doc: ', str(totaltokens / docs))
print('max words in a doc: ', maxds)
print('min words in a doc: ', minds)

from statistics import median
print(median(d.values()))
#this is the median number of times a token occurs in the entire training dataset

trainp = os.listdir(train_path)
totallabels = 0
totalprop = 0
docs = 0
maxprop = -2**30
minprop = 2**30
maxpp = 0
minpp = 1
for i in trainp:
  if i[-4:] == ".txt":
    afn = i[:-4]
    lfn = afn + ".task-si.labels"
    article_file = os.path.join(train_path, i)
    labels_file = os.path.join(train_path, lfn)
    tokens, labels, sanity = master_tokenizer(article_file, labels_file)
    if not sanity:
      print("sanity broke on article", afn)
    else:
      c = 0
      for i in labels:
        c += i
      lenl = len(labels)
      totallabels += lenl
      totalprop += c
      docs += 1
      if maxprop < c:
        maxprop = c
      if minprop > c:
        minprop = c
      if lenl == 0:
        pp = 0
      else:
        pp = c / lenl
      if maxpp < pp:
        maxpp = pp
      if minpp > pp:
        minpp = pp


print('total labels:', str(totallabels))
print('total prop:', str(totalprop))
print('total average prop:', str(totalprop/totallabels))
print('average prop/doc:', str(totalprop / docs))
print('max prop in a doc:', maxprop)
print('min prop in a doc:', minprop) #prop = propaganda words
print('max prop percentage in a doc: {:.2%}'.format(maxpp))
print('min prop percentage in a doc: {:.2%}'.format(minpp))

"""### Q1: Initial data observations
What are your initial observations of the dataset after you explore the dataset?

**Answer:**

about 10% of the dataset is propaganda
There are 251275 labels & tokens with a mean of about 1,000 per article and about 100 propaganda words per article.
The median number of times a word occurs in all of the training data is 2.

Here are some examples of sentences containing propaganda techniques:

the american jewish historical society was caught collaborating with jvp ; a radical anti-israel hate group with links to anti-semitism.

these days it’s instead partnering with jewish voice for peace : an anti-israel bds hate group that defends anti-semitism and which sponsored talks by an anti-semite who accused jews of drinking blood.

he had tweeted , `` eu no longer considers # hamas a terrorist group.

time for us to do same.

# Model 1: HMM Implementation

---

In this section, you will implement a HMM model for this task. We expect:


1. An implementation of the **Viterbi algorithm** that can be used to infer token-level labels --- propaganda or not propaganda --- for an input document.   This process is commonly referred to as **decoding**.
2. Code for counting and smoothing of labels and words as necessary to support the HMM decoding. (This is pretty much what you already know how to do from project 1.)


The tokenization of documents can be performed with the code we provide above. We suggest you calculate probabilities in a log form.  Bigram Viterbi is $ \mathcal{O}(sm^2)$ where s is the length of the sentence and m is the number of tags. Your implementation should have similar efficiency.

Code of Academic Integrity:  We encourage collaboration regarding ideas, etc. However, please **do not copy code from online or share code with other students**. We will be running programmes to detect plagiarism.
"""

# bigram probs
def getLabelTable(trainLabels, k): # 0 is non prop, 1 is prop
  pp = k #prop | prop
  pnp = k #nonprop | prop
  npp = k #prop | nonprop
  npnp = k #nonprop | nonprop
  p = 2*k #prop
  np = 2*k #nonprop
  s = 2*k
  sp, snp, se, pe, npe = k, k, k, k, k
  for lstlabels in trainLabels:
    s += 1
    if len(lstlabels) == 0:
      se += 1
    else:
      sp += lstlabels[0]
      snp += (1-lstlabels[0])
      count = 0
      while count < len(lstlabels)-1:
        p += lstlabels[count]
        np += (1-lstlabels[count])
        if lstlabels[count] == 1 and lstlabels[count+1] == 1:
          pp += 1
        elif lstlabels[count] == 0 and lstlabels[count+1] == 1:
          npp += 1
        elif lstlabels[count] == 1 and lstlabels[count+1] == 0:
          pnp += 1
        elif lstlabels[count] == 0 and lstlabels[count+1] == 0:
          npnp += 1
        else:
          print("error in l31 getlabeltable")
        count += 1
      p += lstlabels[-1]
      np += (1-lstlabels[-1])
      pe += lstlabels[-1]
      npe += (1-lstlabels[-1])
  ''' label table format:
      snp, sp, se
      npnp, npp, npe
      pnp, pp, pe
  '''
  pp /= p;  pnp /= p;  npp /= np;  npnp /= np;  se /= s;  snp /= s;  sp /= s;  pe /= p;  npe /= np;
  tprobs = [[snp, sp, se], [npnp, npp, npe], [pnp, pp, pe]]
  return tprobs

#lexical generation probs
def getWordTable(trainData, trainLabels, k):
  pcount = 0
  npcount = 0
  ltd = len(trainData)
  assert(ltd == len(trainLabels))
  d = {}
  for article in range(ltd):
    for wordIndex in range(len(trainData[article])):
      word = trainData[article][wordIndex]
      label = trainLabels[article][wordIndex]
      if word in d:
        d[word][label] += 1
      else:
        d[word] = [0, 0]
        d[word][label] += 1
      pcount += label
      npcount += (1-label)
  for word in d:
    d1 = (d[word][0] + k)/(npcount + len(d)*k)
    d2 = (d[word][1] + k)/(pcount + len(d)*k)
    d[word][0] = d1
    d[word][1] = d2
  return d

def createLargeSets(trainp):
  trainData = []
  trainLabel = []
  for i in trainp:
    if i[-4:] == ".txt":
      afn = i[:-4]
      lfn = afn + ".task-si.labels"
      article_file = os.path.join(train_path, i)
      labels_file = os.path.join(train_path, lfn)
      tokens, labels, sanity = master_tokenizer(article_file, labels_file)
      assert(sanity)
      trainData.append(tokens)
      trainLabel.append(labels)
  return (trainData, trainLabel)

def precompute(trainData, trainLabels, k):
  wprobs = getWordTable(trainData, trainLabels, k)
  tprobs = getLabelTable(trainLabels, k)
  return (trainData, trainLabels, wprobs, tprobs)

# Your implementation here
# we expect a function or class, mapping a sequence of tokens to a sequence of labels
# this function or class will be called below
def hmm_viterbi(pcdata, article): 
  #assume article is already tokenized
  trainData, trainLabel, wprobs, tprobs = pcdata #pcdata is precompute
  #wprobs is lexical table, tprobs is bigram table
  #initialization
  la = len(article)
  score = [[-1] * la, [-1] * la]
  bptr = [[-1] * la, [-1] * la]
  for i in range(2):
    p1 = tprobs[0][i]
    if article[0] in wprobs:
      p2 = wprobs[article[0]][i]
    else:
      p2 = wprobs["<unk>"][i]
    score[i][0] = (p1*p2)
    bptr[i][0] = 0  
  #iteration
  t = 1
  while t < la:
    for i in range(2):
      p1 = tprobs[1][i]
      if article[t] in wprobs:
        p2 = wprobs[article[t]][i]
      else:
        p2 = wprobs["<unk>"][i]
      npscore = score[0][t-1] * p1 * p2
      p1 = tprobs[2][i]
      pscore = score[1][t-1] * p1 * p2
      if npscore > pscore:
        score[i][t] = npscore
        bptr[i][t] = 0
      else:
        score[i][t] = pscore
        bptr[i][t] = 1
    t += 1
  #identify sequence
  bestpath = [-1] * la
  if score[0][-1] < score[1][-1]:
    bestpath[-1] = 1
  else:
    bestpath[-1] = 0
  i = la-2
  while i >= 0:
    bestpath[i] = bptr[bestpath[i+1]][i+1]
    i -= 1
  return bestpath

"""## Validation Step

1. Create a validation set from the given dataset, i.e. a subset of (~10%) the training dataset that you only use for evaluating the models, not for training.  (You can think of the validation set as a sample test set.)
2. Train your HMM model on the (remainder of the) training set and evaluate it on the validation set. Report **weighted accuracy**, which is explained in the *Notes* section above.

Please also take a look into your misclassified cases, as we will be performing error analysis in the *Evaluation* section. We expect smoothing, unknown word handling and correct emission (i.e., lexical generaion) probabilities. 

In the *Kaggle Submission* section, there is code provided for generating the output file in the form required for Kaggle.  If you find it useful for computing weighted accuracy, you can use it here as well.
"""

# import random
import copy
def addunk(blst2):
  blst = copy.deepcopy(blst2)
  lsts = set()
  count = 0
  for lst in blst:
    for i in range(len(lst)):
      if lst[i] not in lsts:
        lsts.add(lst[i])
        if count % 5 <= 2:
          lst[i] = "<unk>"
        count += .25
  return blst

# This cell splits the training data into a training set and a validation set
trainp = os.listdir(train_path)
trainingset = []
traininglabels = []
validationset = []
validationlabels = []
count = 0
for i in trainp:
  if i[-4:] == ".txt":
    afn = i[:-4]
    lfn = afn + ".task-si.labels"
    article_file = os.path.join(train_path, i)
    labels_file = os.path.join(train_path, lfn)
    tokens, labels, sanity = master_tokenizer(article_file, labels_file)
    assert(sanity)
    if count % 10 == 1:
      count += 1
      validationset.append(tokens)
      validationlabels.append(labels)
    else:
      count += 1
      trainingset.append(tokens)
      traininglabels.append(labels)

pcd = precompute(addunk(trainingset), traininglabels, 0.042819)#1)

t1 = time.time()
vs = []
for i in validationset:
  vs.append(hmm_viterbi(pcd, i))
print('it took', time.time()-t1)

#this cell check for the weighted averages of our model's results
t1 = time.time()
wtot = 0
warr = []
for v in range(len(vs)):
  w = getWA(vs[v], validationlabels[v])
  wtot += w
  warr.append(w)
print('it took', time.time()-t1)
print('avg is', wtot/len(vs))
print('median is', median(warr))

"""#Testing for best k value
Here we can see how we performed our testing for the best value of k.
"""

from statistics import median
def testk(k):
  pcd = precompute(addunk(trainingset), traininglabels, k)
  vs = []
  for i in validationset:
    vs.append(hmm_viterbi(pcd, i))
  wtot = 0
  warr = []
  for v in range(len(vs)):
    w = getWA(vs[v], validationlabels[v])
    wtot += w
    warr.append(w)
    # print(w)
  avg = wtot/len(vs)
  m = median(warr)
  return avg

k = .043
maxavg = 0
maxk = 1234
while k > .042:
  a = testk(k)
  if a > maxavg:
    maxavg = a
    maxk = k
  k -= .00001
print(maxavg, maxk)
# conclusion 10/23 2:26pm: 0.042819 for unk 40%

frac_propaganda = num_propaganda/num_labels   [in the answer key]
weight_propaganda = 1/frac_propaganda  
weight_non_propaganda = 1/(1-frac_propaganda)
 
weighted_accuracy = 
   ((weight_propaganda * # propaganda correct) 
                      +
   (weight_non_propaganda * # non_propaganda correct)) / 
 
   ((weight_propaganda * num_propaganda) 
                      +
   (weight_non_propaganda * num_non_propaganda))

#this cell implements the math formula seen in the previous cell to find the weighted accuracy
def getWA(predicted, answers):
  assert(len(predicted) == len(answers))
  prop_in_pred = 0
  for i in predicted:
    prop_in_pred += i
  num_prop_in_ans_key = 0
  for i in answers:
    num_prop_in_ans_key += i
  frac_p = num_prop_in_ans_key / len(answers)
  if frac_p == 0:
    weight_p = 0
  else:
    weight_p = 1 / frac_p
  weight_np = 1/(1-frac_p)
  
  pc = 0  # propaganda correct
  npc = 0 # non_propaganda correct
  for i in range(len(predicted)):
    if predicted[i] == 1 and answers[i] == 1:
      pc += 1
    if predicted[i] == 0 and answers[i] == 0:
      npc += 1

  wa = ((weight_p * pc) + (weight_np * npc)) / ((weight_p * num_prop_in_ans_key) + (weight_np * (len(answers)-1)))
  return wa

"""### Q2 : Explan your HMM implementations

Q2.1: Explain here how you implemented HMMs (e.g. **which algorithms/data structures** you used). Make clear which parts were implemented from scratch vs. obtained via an existing package. 

**Answer:**
We used MLE with add-k smoothing to get bigram probabilities and stored them in a nested list.
We stored the lexical generation probabilities in a dictionary.
All of this was done from scratch.

Q2.2: Explain and motivate any design choices providing the intuition behind them (e.g. which methods you used for your HMM implementation, why?).

**Answer:**
We chose to use these methods based on discussions in lecutre and the textbook as well as this [youtube video](https://www.youtube.com/watch?v=mHEKZ8jv2SY&t=16s).

### Q3: Results Analysis

Q3.1: Explain here how you evaluated the models. Summarize the performance of your system and any variations that you experimented with on the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis. 

**Answer:**
We evaludated the models using weighted average using a validation set, which was selected from 10% of the training set and comparing the outputs. The HMM achieved best performance of slightly over 50% using k = 0.042819 and when we turned about 40% of first encounters of words into unk.

Q3.2: When did the system work well? When did it fail?  Any ideas as to why? How might you improve the system?

**Answer:**
The system worked well with our best performance values of k and unk. The system failed when we used large values of k or extreme values of unk.

Q3.3: What is the effect of unknown word handling and smoothing?

**Answer:**
Both have a positive effect on our system's performance.

# Model 2: MEMM Implementation

---


In this section, you will implement a Maximum Entropy Markov Model (**MEMM**) to perform the same propaganda detection task. Your model should consist of a MaxEnt classifier with Viterbi decoding. 
 
1. We have already performed tokenizations for documents. You can either use a MaxEnt classifier from an existing package or write the MaxEnt code yourself. **Important note:  MaxEnt classifiers are statistically equivalent to multi-class logistic regression, so you can use packages for multi-class LR instead of MaxEnt.**

2. Use the classifier to learn a probability $P(t_i|features)$. You may replace either the lexical generation probability – $P(w_i|t_i)$ – or the transition probability – $P(t_i|t_{i−1})$ – in the HMM with it, or you may replace the entire *lexical generation probability * transition probability*  calculation – $P (w_i|t_i) ∗ P (t_i|t_{i−1)} – $ in the HMM with it. 

3. To train such classifier, you need to pick some feature set. The content of the feature set is up to your choice. You should be trying different feature sets, and evaluate your choices on the validation set. Pick the feature set that performs overall the best according to the F1 measure.

4. Use your own implementation of the **Viterbi algorithm**, which you can modify from the one you developed for the HMM model. You will need the probabilities that you obtain from the MaxEnt classifier. 

5. Remember to use same training and validation split when evaluating the MEMM to have a **fair comparison** with your **HMM model**.


Please also take a look into your misclassified cases, as we will be performing error analysis in *Evaluation* section.

---
Work flow summary:

![alt text](https://drive.google.com/uc?export=view&id=14VfjW3yDyXLojWM_u0LeJYdDOSLkElBn)
"""

trainp = os.listdir(train_path)
trainData, trainLabel = createLargeSets(trainp)

from nltk.classify import maxent

# Your model implementation here
# we expect a function of class, mapping a sequence of tokens to a sequence of labels
# this function or class will be called below
#
# You will need:
# 1. Extract Features
# 2. Train MaxEnt
# 3. To call Viterbi

nltk.download('averaged_perceptron_tagger')
from nltk import pos_tag

pip install vaderSentiment

import vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def sentiment_scores(sent):
    sentence = ''
    for word in sent:
      sentence+= word + ' '
    # Create a SentimentIntensityAnalyzer object. 
    sid_obj = SentimentIntensityAnalyzer() 
    #grab polarity of sentence
    sentiment_dict = sid_obj.polarity_scores(sentence)
    #return the sentence's overall sentiment
    return sentiment_dict['compound']

def extractFeaturesTraining(trainsetdata, trainsetlabels, sv=1, pv=1, lv=1):
  posd = {'DT' : 1, 'JJ' : 2,'NN' : 3,'NNP' : 4,'VBZ' : 5,'IN' : 6,'TO' : 7,'VB' : 8,
'NNS' : 9,'RB' : 10,'VBP' : 11,'.' : 12,'(' : 13,')' : 14,'PRP' : 15,',' : 16,'PRP$' : 17,
'MD' : 18,'CC' : 19,'<UNK>' : 20,'VBN' : 21,'VBD' : 22,'WP' : 23,'CD' : 24,'VBG' : 25,
'WRB' : 26,':' : 27,'JJS' : 28,'RBR' : 29,'WDT' : 30,'JJR' : 31,'RP' : 32,'EX' : 33,
'PDT' : 34,'FW' : 35,'UH' : 36,'POS' : 37,'``' : 38,"''" : 39,'NNPS' : 40,'$' : 41,
'WP$' : 42,'RBS' : 43,'#' : 44,'SYM' : 45}
  newposset = set()
  memmtrainset = []
  for article1 in range(len(trainsetdata)):
    article = trainsetdata[article1]
    prev_punct = 0
    sent = 0
    pos_tagged = pos_tag(article)
    for token1 in range(len(article)):
      token = article[token1]
      partos = pos_tagged[token1][1]
      if sv:
        if partos == '.':
          sent = sentiment_scores(article[prev_punct:token1+1])
          for j in range(prev_punct,token1):
            memmtrainset[j][0]['sentiment']=sent
          prev_punct = token1+1
        else:
          sent = 0
      if partos in posd:
        if sv and pv and lv:
          d = dict(pos=posd[partos], length=len(token), sentiment = sent)
        elif sv and pv:
          d = dict(pos=posd[partos], sentiment = sent)
        elif sv and lv:
          d = dict(length=len(token), sentiment = sent)
        elif pv and lv:
          d = dict(pos=posd[partos], length=len(token))
        elif lv:
          d = dict(length=len(token))
        elif pv:
          d = dict(pos=posd[partos])
        elif sv:
          d = dict(sentiment = sent)
        else:
          d = {}
          print("error1 in extractfeaturestraining")
      else:
        if partos not in newposset:
          print(partos)
        newposset.add(partos)
        if sv and pv and lv:
          d = dict(pos=posd['<UNK>'], length=len(token), sentiment = sent)
        elif sv and pv:
          d = dict(pos=posd['<UNK>'], sentiment = sent)
        elif sv and lv:
          d = dict(length=len(token), sentiment = sent)
        elif pv and lv:
          d = dict(d = dict(pos=posd['<UNK>']), length=len(token))
        elif lv:
          d = dict(length=len(token))
        elif pv:
          d = dict(pos=posd['<UNK>'])
        elif sv:
          d = dict(sentiment = sent)
        else:
          d = {}
          print("error2 in extractfeaturestraining")
      if trainsetlabels[article1][token1] == 1:
        t = (d, 'y')
      else:
        t = (d, 'x')
      memmtrainset.append(t)
  return memmtrainset

def extractFeaturesTest(article, sv = 1, pv = 1, lv = 1): 
  posd = {'DT' : 1, 'JJ' : 2,'NN' : 3,'NNP' : 4,'VBZ' : 5,'IN' : 6,'TO' : 7,'VB' : 8,
'NNS' : 9,'RB' : 10,'VBP' : 11,'.' : 12,'(' : 13,')' : 14,'PRP' : 15,',' : 16,'PRP$' : 17,
'MD' : 18,'CC' : 19,'<UNK>' : 20,'VBN' : 21,'VBD' : 22,'WP' : 23,'CD' : 24,'VBG' : 25,
'WRB' : 26,':' : 27,'JJS' : 28,'RBR' : 29,'WDT' : 30,'JJR' : 31,'RP' : 32,'EX' : 33,
'PDT' : 34,'FW' : 35,'UH' : 36,'POS' : 37,'``' : 38,"''" : 39,'NNPS' : 40,'$' : 41,
'WP$' : 42,'RBS' : 43,'#' : 44,'SYM' : 45}
  newposset = set()
  memmtestset = []
  prev_punct = 0
  sent = 0
  pos_tagged = pos_tag(article)
  for token1 in range(len(article)):
    token = article[token1]
    partos = pos_tagged[token1][1]
    if sv:
      if partos == '.':
        sent = sentiment_scores(article[prev_punct:token1+1])
        for j in range(prev_punct,token1):
          memmtestset[j]['sentiment']=sent
        prev_punct = token1+1
      else:
        sent = 0
    if partos in posd:
      if sv and pv and lv:
        d = dict(pos=posd[partos], length=len(token), sentiment = sent)
      elif sv and pv:
        d = dict(pos=posd[partos], sentiment = sent)
      elif sv and lv:
        d = dict(length=len(token), sentiment = sent)
      elif pv and lv:
        d = dict(pos=posd[partos], length=len(token))
      elif lv:
        d = dict(length=len(token))
      elif pv:
        d = dict(pos=posd[partos])
      elif sv:
        d = dict(sentiment = sent)
      else:
          d = {}
          print("error1 in extractfeaturestest")
    else:
      if partos not in newposset:
        print(partos)
      newposset.add(partos)
      if sv and pv and lv:
        d = dict(pos=posd['<UNK>'], length=len(token), sentiment = sent)
      elif sv and pv:
        d = dict(pos=posd['<UNK>'], sentiment = sent)
      elif sv and lv:
        d = dict(length=len(token), sentiment = sent)
      elif pv and lv:
        d = dict(d = dict(pos=posd['<UNK>']), length=len(token))
      elif lv:
        d = dict(length=len(token))
      elif pv:
        d = dict(pos=posd['<UNK>'])
      elif sv:
        d = dict(sentiment = sent)
      else:
          d = {}
          print("error2 in extractfeaturestest")
    memmtestset.append((d))
  return memmtestset

def getprobs(classifier, testset):
  probs = []
  for fs in testset:
    pdist = classifier.prob_classify(fs)
    p = []
    p.append(pdist.prob('x'))
    p.append(pdist.prob('y'))
    probs.append(p)
  return probs

# 3. To call Viterbi
def MEMMviterbi(wprobs, article, mprobs):
  tempp2 = 0
  #initialization
  la = len(article)
  score = [[-1] * la, [-1] * la]
  bptr = [[-1] * la, [-1] * la]
  for i in range(2):
    p1 = mprobs[0][i]
    if article[0] in wprobs:
      p2 = wprobs[article[0]][i]
      tempp2 = p2
    else:
      p2 = tempp2
    score[i][0] = p1
    bptr[i][0] = 0
  #iteration
  t = 1
  for i in mprobs:
    if len(i) != 2:
      print('mprobbroke12345')
  while t < la:
    for i in range(2):
      if t < 0 or t > la:
        print("t broke", t)
      if i<0 or i>1:
        print('i broke', i)
      p1 = mprobs[t][i]
      if article[t] in wprobs:
        p2 = wprobs[article[t]][i]
        tempp2 = p2
      else:
        p2 = tempp2
      npscore = score[0][t-1] * p1
      pscore = score[1][t-1] * p1
      if npscore > pscore:
        score[i][t] = npscore
        bptr[i][t] = 0
      else:
        score[i][t] = pscore
        bptr[i][t] = 1
    t += 1
  #identify sequence
  bestpath = [-1] * la
  print(score)
  if score[0][-1] < score[1][-1]:
    bestpath[-1] = 1
  else:
    bestpath[-1] = 0
  i = la-2
  while i >= 0:
    bestpath[i] = bptr[bestpath[i+1]][i+1]
    i -= 1
  return bestpath

# Run your model on validation set
# You will need to 
# 1. Call your function above to get a prediction result on Validation Set
# 2. Report Metrics
# (See if you need to modify your feature set)

# Run your model on validation set
# You will need to 
# 1. Call your function above to get a prediction result on Validation Set
# 2. Report Metrics
# (See if you need to modify your feature set)

#this cell splits the training data into a training set and a validation set
trainp = os.listdir(train_path)
trainingset = []
traininglabels = []
validationset = []
validationlabels = []
count = 0
for i in trainp:
  if i[-4:] == ".txt":
    afn = i[:-4]
    lfn = afn + ".task-si.labels"
    article_file = os.path.join(train_path, i)
    labels_file = os.path.join(train_path, lfn)
    tokens, labels, sanity = master_tokenizer(article_file, labels_file)
    assert(sanity)
    if count % 10 == 1:
      count += 1
      validationset.append(tokens)
      validationlabels.append(labels)
    else:
      count += 1
      trainingset.append(tokens)
      traininglabels.append(labels)

# this cell was an outline for how we would train then run our MEMM
t1 = time.time()
vs = []
wprobs = getWordTable(trainingset, traininglabels, 0)
eftrain = extractFeaturesTraining(trainingset, traininglabels)
print('pre train')
classifier = nltk.classify.MaxentClassifier.train(eftrain, 'GIS', trace=0, max_iter=15)
print('pre go')
for i in validationset:
  eftest = extractFeaturesTest(i)
  mprobs = getprobs(classifier, eftest)
  v = MEMMviterbi(wprobs, i, mprobs)
  vs.append(v)
print('it took', time.time()-t1)
for i in range(len(vs)):
  print(vs[i])
  print(validationlabels[i])
print('time', time.time()-t1)

def doMEMM():
  t1 = time.time()
  vs = []
  wprobs = getWordTable(trainingset, traininglabels, 0)
  eftrain = extractFeaturesTraining(trainingset, traininglabels)
  print('pre train')
  classifier = nltk.classify.MaxentClassifier.train(eftrain, 'GIS', trace=0, max_iter=15)
  print('pre go')
  for i in validationset:
    eftest = extractFeaturesTest(i)
    mprobs = getprobs(classifier, eftest)
    v = MEMMviterbi(wprobs, i, mprobs)
    vs.append(v)
  print('time', time.time()-t1)
  return vs

memmvs = doMEMM()

#this cell makes use of the getWA() function created for checking the accuracy
#of our HMM model and checks the accuracy of our MEMM model
t1 = time.time()
wtot = 0
warr = []
for v in range(len(memmvs)):
  w = getWA(memmvs[v], validationlabels[v])
  wtot += w
  warr.append(w)
  # print(w)
print('it took', time.time()-t1)
print('avg is', wtot/len(memmvs))
print('median is', median(warr))

"""#Testing for best featureset to use
Below, we can see the method used to test the best combination of the three features we tried using (sentiment, parts of speech, and token length). 
"""

def bestMEMM():
  t1 = time.time()
  max_avg = -1
  vals = ()
  wprobs = getWordTable(trainingset, traininglabels, 0)
  for s in range(2):
    for p in range(2):
      for l in range(2):
        if s==0 and p==0 and l==0:
          pass
        else:
          vs = []
          eftrain = extractFeaturesTraining(trainingset, traininglabels, s, p, l)
          # print('pre train')
          classifier = nltk.classify.MaxentClassifier.train(eftrain, 'GIS', trace=0, max_iter=15)
          # print('pre go')
          for i in validationset:
            eftest = extractFeaturesTest(i, s, p, l)
            mprobs = getprobs(classifier, eftest)
            v = MEMMviterbi(wprobs, i, mprobs)
            vs.append(v)
          wtot = 0
          warr = []
          for v in range(len(vs)):
            w = getWA(vs[v], validationlabels[v])
            wtot += w
            warr.append(w)
          avg = wtot/len(vs)
          if avg>max_avg:
            max_avg = avg
            vals = (s, p, l)
  print('it took', time.time()-t1)
  print ('best average was', max_avg,' with values of', vals)
  return (max_avg, vals)

dobest = bestMEMM()
print(dobest)

"""#Error analysis for MEMM"""

memmvs = doMEMM()

for i in range(len(memmvs[3])):
  if validationlabels[3][i] != memmvs[3][i]:
    print('validation', validationlabels[3][i], 'predicted', memmvs[3][i], 'word', validationset[3][i])
    print()

"""#Error analysis for HMM"""

pcd = precompute(addunk(trainingset), traininglabels, 0.042819)#1)

for i in validationset:
  vs.append(hmm_viterbi(pcd, i))

for i in range(len(vs[3])):
  if validationlabels[3][i] != vs[3][i]:
    print('validation', validationlabels[3][i], 'predicted', vs[3][i], 'word', validationset[3][i])
    print()

"""### Q4: Implementation Details

Q4.1: Explain here how you implemented the MEMM (e.g. which algorithms/data structures you used, what features are included). Make clear which parts were implemented from scratch vs. obtained via an existing package. 

**Answer:**
Our implementation of MEMM uses sentiment and parts of speech. We attempted to use length of token, but found that it decreased accuracy. Our implementation of sentiment used the SentimentIntensityAnalyzer from the vaderSentiment package. Our implementation of parts of speech used the pos_tag tagger from the averaged_perceptrong_tagger package of nltk. Sentiment of each word was captured sentence by sentence and stored in a list. Parts of speech was stored in lists. These were both put into a list of tuples of dictionaries that could be fed into nltk.classify's maxent classifier.

Q4.2: What features are considered most important by your MaxEnt Classifier? Why do you think these features make sense? Describe your experiments with feature sets.

**Answer:**
Our features are considered to be equally important by our classifier. These features make sense because we believe that propoganda is more likely to have a negative sentiment and favor certain types of speech over others. Our experiments with the use of our feature sets included replacing just the lexical table with the MEMM classifier probabilties, replacing the bigram table, and replacing both. We found that replacing both gave us the best performance.

### Q5: Results Analysis

Q5.1: Explain here how you evaluated the MEMM model. Summarize the performance of your system and any variations that you experimented with the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis. 

**Answer:**
We evaluated the model using weighted average using a validation set, which wa sselected from 10% of the training set and comparing the outputs. The MEMM model achieved best performance of 51.5% using the sentiment and parts of speech features. Our experiments with feature sets involved testing each possible combination of the three featureset we tried using until we discovered that the best combination was to use only sentiment and parts of speech.

Q5.2: An analysis on feature selection for the MEMM is required – e.g. what features **help most**, why? An **error analysis** is required – e.g. what sorts of errors occurred, why? 

**Answer:**
The parts of speech feature helped the most. We found this during our testing of the three feature sets. The reason for this is likely that the common parts of speech used in propaganda speech was more different from non-propaganda speech than the difference between the sentiment analysis of the two types of speech. One error we found during our testing was that parts of speech that we had not included in our library would sometimes be returned since we could not find a complete set of all the parts of speech. This was easily fixed.
In our analysis, we found that confusing sentences and sentences with large amounts of negative sentiments seem to be incorrectly predicted more often. Since sentiment runs on the sentence level, we believe that it takes in words like "safe", which causes positive sentiment of the sentence to be errounsly weighted too highly. This throws off the sentiment of the sentence and we believe that this decreases the accuracy of the system. One piece of evidence we found to strongly support this belief while working on the implementation was that if we made the sentiment feature more extreme (-1 for negative and 1 for positive, as opposed to the only the overall), the system accuracy was worse.

Q5.3: When did the system work well, when did it fail and any ideas as to why? How might you improve the system?

**Answer:**
The system worked well when it was able to correctly guess the sentiment analysis. Our thoughts on this were described previously. If we were able to correct the sentiment analysis, our system might perform better.

#Comparing HMMs and MEMMs

---


### Q6:


Compare here your results for your HMM and the MEMM. Which of them performs better? 

**Answer:**
Our MEMM performs slightly better.

###Q7: Error Analysis
Q7.1: Do some error analysis. What are error patterns you observed that the HMM makes but the MEMM does not? Try to justify why/why not?

**Answer:**
The HMM model performs worse when there are error in grammar and spelling compared to MEMM. These errors may be cause by tokenization, we are unsure. We believe that HMM does worse than MEMM on these points because MEMM takes into account parts of speech while HMM tried to use the lexical and bigram tables. These tables would perform much more poorly on rare or unknown words than MEMM.

Q7.2: What are error patterns you observed that MEMM makes but the HMM does not? Try to justify what you observe?

**Answer:**
The MEMM makes errors when sentiments are incorrectly classified while the HMM does not. This is for the obvious reason that MEMM takes into account sentiment analysis while HMM does not.

# Kaggle Submission 
---

Using the best-performing system from among all of your HMM and MEMM models, generate predictions for the test set, and submit them to Kaggle at https://www.kaggle.com/t/8a8030baefcc4d91b715f114353dba38. Note, you **need** to use our tokenizer as the labels on Kaggle corresponds to these. Below, we provide a script that submits a random guess/all ones or all zeroes in the correct format. Note that you only need to provide a function which takes as input a sequence of tokens, and outputs a sequence of labels (in the form of integers  in {0,1}, 1 corresponding to propaganda). As a scoring metric on Kaggle, we use **weighted accuracy**, described in the *Notes* section towards the beginning of the notebook.
"""

import random
from typing import Callable


def submission_to_csv(binary_class : List[List[int]], fname : str):
  # writes submission to CSV
  new_binary = []
  for classes in binary_class:
    new_binary += classes
  with open(fname, 'w') as file:
    file.write('id,category\n')
    for i, line in enumerate(new_binary):
      file.write('{},{}\n'.format(i,line))


def random_predictor(article_tokens : List[str]) -> List[int]:
  # random predictor
  output = []
  for _ in range(len(article_tokens)):
    output.append(random.randint(0,1))
  return output

def always_one(article_tokens : List[str]) -> List[int]:
  # random predictor
  output = []
  for _ in range(len(article_tokens)):
    output.append(1)
  return output

def always_zero(article_tokens : List[str]) -> List[int]:
  # random predictor
  output = []
  for _ in range(len(article_tokens)):
    output.append(0)
  return output


def generate_submission(predictor: Callable[[List[str]], List[int]], fname : str, test_path : str):
  # generate a submission with random guesses
  sample_submission = []
  articles = os.listdir(test_path)
  total, fail, targets, test_articles = 0,0, [], []
  for article in sorted(articles):
    article_file = os.path.join(test_path, article)
    art_toks = tokenize_article(article_file)
    curr_article = predictor(art_toks)
    sample_submission.append(curr_article)
  submission_to_csv(sample_submission, fname)


generate_submission(random_predictor, "random_submission.csv", test_path)
generate_submission(always_one, "ones_submission.csv", test_path)
generate_submission(always_zero, "zeros_submission.csv", test_path)

#MEMM has a separate generate submission function so that we can pass in our trained
#classifier rather than retraining the classifier for each article
def generate_submissionMEMM(predictor: Callable[[List[str]], List[int]], fname : str, test_path : str, wprobs, classifier):
  # generate a submission with random guesses
  sample_submission = []
  articles = os.listdir(test_path)
  total, fail, targets, test_articles = 0,0, [], []
  for article in sorted(articles):
    article_file = os.path.join(test_path, article)
    art_toks = tokenize_article(article_file)
    curr_article = predictor(art_toks, wprobs, classifier)
    sample_submission.append(curr_article)
  submission_to_csv(sample_submission, fname)

def genHMM(article_tokens : List[str]) -> List[int]:
  # HMM predictor
  trainp = os.listdir(train_path)
  trainData, trainLabel = createLargeSets(trainp)
  pcd = precompute(addunk(trainData), trainLabel, 0.042819)
  output = hmm_viterbi(pcd, article_tokens)
  return output

def genMEMM(article_tokens : List[str], wprobs, classifier) -> List[int]:
  # MEMM predictor
  eftest = extractFeaturesTest(article_tokens, 1, 1, 0)
  mprobs = getprobs(classifier, eftest)
  v = MEMMviterbi(wprobs, article_tokens, mprobs)
  return v

generate_submission(genHMM, "hmm_submission.csv", test_path)

#This code extracts features from the training data and trains the classifier for the MEMM model
trainp = os.listdir(train_path)
trainData, trainLabel = createLargeSets(trainp)
wprobs = getWordTable(trainData, trainLabel, 0)
eftrain = extractFeaturesTraining(trainData, trainLabel, 1, 1, 0)
classifier = nltk.classify.MaxentClassifier.train(eftrain, 'GIS', trace=0, max_iter=15)
print('done')

t1 = time.time()
generate_submissionMEMM(genMEMM, "memm_submission.csv", test_path, wprobs, classifier)
print('time', time.time()-t1)

"""---
### Q8: Competition Score

Include here your **team name** and the **screenshot** of your best score from Kaggle.

**Answer:**
Our team name is wh395_pjc272
![chrome_W0HVOQf6hj.png](data:image/png;base64,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)

#Appendix
---

### Tentative grading guideline

- (5 pts) Dataset exploration.
- (20 pts) Design and implementation of the HMM model. Specifically, calculations of emission/transition probability, smoothing, handling of unknown words.
- (20 pts) Design and implementation of the MEMM, as well as the feature set and how the feature extraction is done.
- (20 pts) Experiment design and methodology. Investigations into feature sets and unknown word handling/smoothing.
- (20 pts) Error analysis and comparison of **HMM** model with the **MEMM**. 
- (10 pts) Report: organization, clarity and including the **corresponding pieces of code for the implementations**.
- (5 pts) **Submission to Kaggle**. In the report, you should add a screenshot of your team’s performance on kaggle leaderboard.
"""